{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>PRI 2022: first project delivery</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GROUP X**\n",
    "- name, number\n",
    "- name, number\n",
    "- name, number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Part I: demo of facilities</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A) Indexing facilities**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*A.1 Preprocessing options: statistics*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: parsing empty text\n",
      "avg sentences 7.893483146067416\n",
      "avg characters 2232.4188764044943\n"
     ]
    }
   ],
   "source": [
    "#rare, most common terms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*A.2 Indexing statistics*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here\n",
    "#time, memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B) Ranking facilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: parsing empty text\n"
     ]
    }
   ],
   "source": [
    "# load files\n",
    "from mainFunctions.ranking import ranking\n",
    "from operator import attrgetter\n",
    "from helper.documentHelper import read_files\n",
    "from mainFunctions.indexing import indexing\n",
    "order_ranked = True\n",
    "text_processing = True\n",
    "documents = read_files(text_processing, [\"business\"])\n",
    "\n",
    "corpus_index = indexing(list(map(attrgetter('text_terms'), documents)))\n",
    "corpus_idfs: {str: float} = {}\n",
    "\n",
    "for v in corpus_index:\n",
    "    corpus_idfs[v] = corpus_index[v].inverted_document_frequency\n",
    "\n",
    "document = documents[0]\n",
    "\n",
    "summary1 = ranking(document, 7, None, order_ranked, corpus_idfs, {\"rank_option\": \"tf\", \"mmr\": True})\n",
    "summary2 = ranking(document, 3, None, order_ranked, corpus_idfs, {\"rank_option\": \"tf-idf\", \"mmr\": False})\n",
    "summary3 = ranking(document, 3, None, order_ranked, corpus_idfs, {\"rank_option\": \"bm25\", \"mmr\": False})\n",
    "summary4 = ranking(document, 3, None, order_ranked, corpus_idfs, {\"rank_option\": \"rrf\", \"mmr\": False})\n",
    "summary5 = ranking(document, 3, None, order_ranked, corpus_idfs, {\"rank_option\": \"tf-idf\", \"mmr\": True})\n",
    "\n",
    "document.summary = summary5\n",
    "\n",
    "document_sentences = document.text_sentences\n",
    "summary_sentences = document.summary\n",
    "reference_summary_sentences = document.referenceSummary"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B.1 Summarization solution: results for a given document*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B.2 Classic IR models: differences*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B.3 Reciprocal rank funsion*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*B.4 Maximal Marginal Relevance*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C) Sentence higlighting facilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D) Evaluation facilities**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*D.1 Evaluation options*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*D.2 Comparison of settings (IR models, preprocessing)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Part II: questions materials (optional)</H3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** Corpus and summary description. Distribution of informative terms before and after text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here\n",
    "\n",
    "from src.helper.documentHelper import read_files\n",
    "\n",
    "\n",
    "documents_no_preprocessing = read_files(False)\n",
    "documents_preprocessing = read_files(True)\n",
    "\n",
    "print(documents_preprocessing[0].text)  #Test\n",
    "\n",
    "# Calculate distribution of informative termsstics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Summarization performance for the overall and category-conditional corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code, statistics and/or charts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**...** (additional questions with empirical results)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<H3>Part III: Other caluclations to support the project</H3>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg sentences 7.893483146067416\n",
      "avg characters 1008.9977528089887\n"
     ]
    }
   ],
   "source": [
    "# get average sentences and characters of the reference summaries\n",
    "from helper.documentHelper import read_files\n",
    "totalSentences = 0\n",
    "totalCharacters = 0\n",
    "totalDocuments = 0\n",
    "documents = read_files(False)\n",
    "\n",
    "for document in documents:\n",
    "    sentences = document.referenceSummary\n",
    "    totalSentences += len(sentences)\n",
    "    totalCharacters += len(\" \".join(sentences))\n",
    "    totalDocuments +=1\n",
    "\n",
    "print(\"avg sentences\", totalSentences / totalDocuments)\n",
    "print(\"avg characters\", totalCharacters /totalDocuments)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>END</H3>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
